{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11443,"status":"ok","timestamp":1728030583324,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"-PST1zDLX8Qi","outputId":"a2207a10-fadd-4ca7-f3c2-cfd3f3a92756"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1728030884884,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"ImhGamN4YJGv","outputId":"7858ada3-a286-425e-e079-2b0705081706"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Project-X-Lip-Reading\n"]}],"source":["%cd /content/drive/MyDrive/Project-X-Lip-Reading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ1_3rGTYTu5"},"outputs":[],"source":["import numpy as np\n","import os\n","import imutils\n","import dlib\n","import cv2\n","import matplotlib.pyplot as plt\n","import skimage\n","from skimage.transform import resize\n","import imageio\n","from imutils import face_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLsy8deqY69L"},"outputs":[],"source":["words = ['NULL', 'Begin', 'Choose', 'Connection', 'Navigation', 'Next', 'Previous', 'Start', 'Stop', 'Hello', 'Web']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSH6KB-kZC7x"},"outputs":[],"source":["get_ipython().run_line_magic('pwd', '')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhRq8BzHPIwK"},"outputs":[],"source":["def face_extractor(img):\n","    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    face_classifier = cv2.CascadeClassifier('/content/drive/MyDrive/Project-X-Lip-Reading/Dataset Preprocessing/xml files/haarcascade_frontalface_default.xml')\n","    faces = face_classifier.detectMultiScale(image, 1.3, 5)\n","\n","    # If faces are found, extract the first face\n","    if len(faces) \u003e 0:\n","        for (x, y, w, h) in faces:\n","            cropped_image = image[y:y+h, x:x+w]\n","            cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)\n","            return cropped_image\n","    else:\n","        print(\"No face found.\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb8iN9WUPTOV"},"outputs":[],"source":["def lips_extractor(img):\n","    predictor = dlib.shape_predictor('/content/drive/MyDrive/Project-X-Lip-Reading/Dataset Preprocessing/xml files/shape_predictor_68_face_landmarks.dat')\n","\n","    image = imutils.resize(img, width=56)\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    bbox = dlib.rectangle(0, 0, gray.shape[1], gray.shape[0])\n","    face_landmarks = predictor(gray, bbox)\n","    face_landmarks = face_utils.shape_to_np(face_landmarks)\n","\n","    for (name,(i,j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n","         if name=='mouth':\n","            for (x, y) in face_landmarks[i:j]:\n","                (x, y, w, h) = cv2.boundingRect(np.array([face_landmarks[i:j]]))\n","                lip_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n","                lip_image = imutils.resize(lip_image, width=500, inter=cv2.INTER_CUBIC)\n","\n","                lip_image = cv2.cvtColor(lip_image, cv2.COLOR_BGR2GRAY)\n","\n","    if len(lip_image) == 0:\n","        print(\"No lips detected.\")\n","        return None\n","    else:\n","        return lip_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEpH7J-ijvTP"},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","frames = []\n","recording = False\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    key = cv2.waitKey(1)\n","    if key == 27:  # Escape key to exit\n","        break\n","    elif key == 32:  # Spacebar to start/stop recording\n","        recording = not recording\n","\n","    if recording:\n","        frames.append(frame)\n","        cv2.putText(frame, \"Recording...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n","    else:\n","        cv2.putText(frame, \"Press Space to Record\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n","\n","    cv2.imshow(\"Video\", frame)\n","\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","print(f\"Recorded {len(frames)} frames.\")\n","\n","min_frames = 8\n","max_frames = 28\n","total_frames = len(frames)\n","\n","# Calculate the number of frames to extract\n","num_frames = min(max_frames, max(min_frames, total_frames // 10))\n","\n","# Calculate the interval between frames\n","interval = total_frames // num_frames\n","\n","sequence = []\n","for i in range(num_frames):\n","    frame = frames[i * interval]\n","    frame = face_extractor(frame)\n","    frame = lips_extractor(frame)\n","    frame = imutils.resize(frame, width=100)\n","    frame = 255 * frame\n","    frame = frame.astype(np.uint8)\n","    sequence.append(frame)\n","\n","# Normalize the sequence\n","sequence = np.array(sequence)\n","v_min = sequence.min(axis=(1, 2), keepdims=True)\n","v_max = sequence.max(axis=(1, 2), keepdims=True)\n","sequence = (sequence - v_min) / (v_max - v_min)\n","sequence = np.nan_to_num(sequence)\n","\n","print(f\"Processed {len(sequence)} frames.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1728029354642,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"jy4QCJmwSKWa"},"outputs":[],"source":["def display_images(image_list):\n","    fig, axes = plt.subplots(2, 14, figsize=(14, 2))\n","    for i, img in enumerate(image_list):\n","        row, col = divmod(i, 14)\n","        axes[row, col].imshow(img)\n","        axes[row, col].axis('off')  # Hide axes\n","    plt.show()\n","\n","display_images(sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1728029354643,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"T6vy_xbqoSHD"},"outputs":[{"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-79fc5f6aeaf6\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}],"source":["sequence.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1728029354643,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"fGbCJ6qQksWj"},"outputs":[],"source":["import tensorflow as tf\n","\n","# Load the model\n","loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Project-X-Lip-Reading/Model Architecture/Saved Model/3D_CNN_LSTM.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1728029354643,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"1zv5WhNKk-SJ"},"outputs":[],"source":["# Normalize the sequence\n","np.seterr(divide='ignore', invalid='ignore')  # Ignore divide by 0 warning\n","v_min = sequence.min(axis=(1, 2), keepdims=True)\n","v_max = sequence.max(axis=(1, 2), keepdims=True)\n","sequence = (sequence - v_min) / (v_max - v_min)\n","sequence = np.nan_to_num(sequence)\n","\n","# Reshape the input for prediction\n","my_pred = sequence.reshape(1, 28, 100, 100, 1)\n","ans = loaded_model.predict(my_pred)\n","\n","# Get all words with their percentages\n","percentages = [round(p * 100, 2) for p in ans[0]]\n","predictions = {words[i]: percentages[i] for i in range(len(words))}\n","\n","# Print all words with their percentages\n","for word, percent in predictions.items():\n","    print(f\"Predicted: {word} , {percent} %\")\n","\n","max_index = np.argmax(ans)\n","text = f\"Predicted: {words[max_index]} , {percentages[max_index]} %\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1728029354644,"user":{"displayName":"Om Mukherjee","userId":"12080237491388735542"},"user_tz":-330},"id":"pCRHsf8DpRwc"},"outputs":[],"source":["print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ED2naj8jpWp4"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}