### 2. Batch Normalization

- When training a model, we normalize the input features by subtracting the mean and dividing by the standard deviation.
- Batch normalization extends the idea of normalizing *input features* to normalize the mean and variance of the *hidden layer* activations.
- Batch normalization helps in training deep neural networks more efficiently by standardizing the mean and variance of hidden layer activations. It improves the performance and stability of the network.

1. **Compute the mean:** `μ = (1/n) * Σxi`
2. **Compute the standard deviation:** `σ = sqrt((1/n) * Σ(xi - μ)^2)`
3. **Normalize the values:** `(x - μ) / (σ + ε)`
4. **Scale and shift the normalized values:** γ * x_normalized + β

*γ and β are parameters here*

![batch norm](https://github.com/user-attachments/assets/7faff850-7918-47ac-8642-093a9ce4abec)

#### 2.1 Fitting batch norm into neural network 

1. The scaled and shifted values are passed through an activation function, such as the sigmoid or ReLU function.
2. The normalized and activated values are propagated forward through the network, passing through each layer and repeating the steps of Batch Normalization.
3. During the training process, the network calculates the gradients of the loss function with respect to the parameters. 
4. Batch Normalization also calculates the gradients for the Gamma and Beta parameters, allowing them to be updated as well.

*During backpropagation in Batch Normalization, we calculate the gradients for the weights (W), scaling (Gamma), and shifting (Beta) parameters. We don't need to calculate the gradient for the bias term (B) because it gets canceled out during the normalization step.*

![Batch Norm fitting](https://github.com/user-attachments/assets/99b9b5a7-05c1-473d-a1bb-449a60c89037)

![working with mini batches](https://github.com/user-attachments/assets/66e1e464-4742-4fc8-a13f-803963e42d6c)

#### 2.2 Implementing gradient descent 

![implementing](https://github.com/user-attachments/assets/d59252ef-4ec2-4d5d-9b93-4392df3987f6)

#### 2.3 Why does batch normalization works?

1. **Normalizing input features:** Just like how normalizing the input features (X's) to have a mean of zero and a variance of one can speed up learning, batch normalization does a similar thing but for the hidden units in the network.
2. **Handling covariate shift:** Covariate shift refers to the situation where the distribution of the input data changes. Batch normalization helps address this problem by reducing the amount that the distribution of the hidden unit values shifts around.

![why batch norm works](https://github.com/user-attachments/assets/23b33da7-6a0d-40be-9b39-fa9a673e15fd)
