{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vectorization\n",
    "\n",
    "- Vectorization is a technique that allows us to perform operations on entire arrays or matrices instead of looping through each element individually.\n",
    "\n",
    "- In a vectorized implementation, you can directly compute the dot product without using a for loop. This is done using built-in functions like np.dot() in Python or numpy. \n",
    "\n",
    "![Vectorizing logistic regression](https://github.com/user-attachments/assets/2ec0a66d-d450-4d12-936f-02fbb7a1012c)\n",
    "\n",
    "#### 1.1 More Examples\n",
    "\n",
    "1. Matrix multiplication\n",
    "2. Element-wise operartions\n",
    "3. Mathematical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized version: 2.9947757720947266 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Loop: 981.5566539764404 ms\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(\"For Loop: \" + str(1000*(toc-tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Vectorizing Logistic Regression\n",
    "\n",
    "- We stack our training inputs together in a matrix called X, and then we can compute all the Z values at once using a single line of code. \n",
    "\n",
    "- Similarly, we can compute all the activation values at once using another line of code. This saves us a lot of time and makes our code much faster.\n",
    "\n",
    "#### 2.1 Gradient Output\n",
    "\n",
    "![Logistic regression gradient output](https://github.com/user-attachments/assets/84040576-93ca-4500-93f4-8a91791394dc)\n",
    "\n",
    "![Logistic regression gradient output2](https://github.com/user-attachments/assets/5f636650-e8e5-4adf-8019-9135a1f107cc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Broadcasting in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.    0.    4.4  68. ]\n",
      " [  1.2 104.   52.    8. ]\n",
      " [  1.8 135.   99.    0.9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = np.array([[56.0, 0.0, 4.4, 68.0], \n",
    "              [1.2, 104.0, 52.0, 8.0], \n",
    "              [1.8, 135.0, 99.0, 0.9]])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59.  239.  155.4  76.9]\n"
     ]
    }
   ],
   "source": [
    "cal = A.sum(axis = 0)\n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.91525424  0.          2.83140283 88.42652796]\n",
      " [ 2.03389831 43.51464435 33.46203346 10.40312094]\n",
      " [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "percentage = 100*A/cal.reshape(1,4)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalization\n",
    "\n",
    "- Normalization often leads to a better performance because gradient descent converges faster after normalization. \n",
    "- By normalization we mean changing x to  `ùë•‚Äñùë•‚Äñ`\n",
    "(dividing each row vector of x by its norm).\n",
    "\n",
    "`‚Äñùë•‚Äñ=np.linalg.norm(x, axis=1, keepdims=True)=[5/‚àö56]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 3., 4.],\n",
    "              [1., 6., 4.]])\n",
    "x_norm = np.linalg.norm(x,keepdims = True,ord=2, axis = 1)\n",
    "x /= x_norm\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vectorizing across multiple examples\n",
    "\n",
    "![vectorizing across example](https://github.com/user-attachments/assets/e910a67d-e6c4-48fd-baee-3f2ee44ea5c2)\n",
    "\n",
    "![vectorizing across 2](https://github.com/user-attachments/assets/272c8892-d73f-49b7-829f-5e43a565e438)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
