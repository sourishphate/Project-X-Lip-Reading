## Neural Network

### 1. Neural Network Representation

- The first layer is called the input layer, and it contains the input features or data that we want to feed into the network.
- The second layer is called the hidden layer, and it is where the network processes and learns from the input data.
- The final layer is called the output layer, and it gives us the predicted value or result.

![NN representation](https://github.com/user-attachments/assets/42e33d3b-908d-4584-bc11-dc655e31bf20)

_Each node in the hidden layer performs two steps of computation._

1. It calculates a value called "z" using a formula that involves weights, inputs, and biases.
2. it applies a sigmoid function to the value of "z" to get the final output called "a".

![neural rep 2](https://github.com/user-attachments/assets/47eb7169-47f5-4106-bbe8-43d7a2d30a7f)

### 4. Activation Function

- It determines how the network processes and outputs information.
- The most commonly used activation function is the sigmoid function, which gives outputs between 0 and 1.
- One popular alternative is the hyperbolic tangent function (tan h), which gives outputs between -1 and 1.
- Another popular choice is the rectified linear unit (ReLU), which gives outputs equal to the input if it's positive and 0 if it's negative.

```
Binary Classification -> Sigmoid Function
Others -> ReLU / Leaky ReLU or Tanh Function
```

_Activation function can be different for different layers_

![activation func](https://github.com/user-attachments/assets/d2d0784d-37ac-40b7-9d58-78fa45e0a1ab)

_Why do neural network needs non-linear activation function?_

- Without a non-linear activation function, the neural network would only be able to compute linear functions.
- In case of linear activation function, the output of the neural network would be a linear function of the input, which is not very useful.

#### 2.1 Derivatives of Activation Function

1. Sigmoid Function: `g(z)*(1 - g(z))`
2. Hyperbolic Tangent (Tanh): `1 - g(z)^2`
3. ReLU and Leaky ReLU: `0 or 1`

### 3. Gradient Descent for Neural Network

![Gradient descent in nn](https://github.com/user-attachments/assets/ce9ac211-4f39-47d3-8cbd-7edb55dc5d48)

#### 3.1 Random Initialization

- It is important to initialize the weights randomly rather than setting them all to zero.
- If you initialize the weights to all zeros, both hidden units will compute the same function.
- So, there is no point in having more than one hidden unit if they are all doing the same thing.
- To solve this problem, we initialize the weights randomly.
- This allows the neural network to learn and capture different features from the input data.
- Initializing weights randomly in neural networks helps to break the symmetry between hidden units and allows them to compute different functions
