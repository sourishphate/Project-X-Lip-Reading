# What is regularization?
- Regularization is a method used in machine learning to deal with the problem of overfitting.
-  A situation in machine learning where a model learns the training data too well, including its noise and outliers, resulting in a model that performs well on the training data but poorly on new, unseen data. This happens when the model is too complex .

#### Regularization Parameter (λ):
- Controls how much regularization is applied.
- Selection: Chosen through cross-validation to balance between good performance on the training set and preventing overfitting.
- Python Note: While using, λ is often written as lambd to avoid conflicts with Python's lambda.

## Regularization in Logistic Regression:
-  In logistic regression, the cost function is used to measure the difference between the predicted and actual values. It includes the sum of losses over all training examples.
-  To add regularization, an additional term is introduced to the cost function. This term involves a regularization parameter (lambda) and the norm of the weights (W).
- L2 Regularization: In L2 regularization, the additional term is the squared norm of the weights. This helps to keep the weights small and prevents the model from overfitting the training data.

![Reg in NN](https://github.com/user-attachments/assets/675ecf98-5264-4a4a-adbb-75b4c1b95091)

## Regularization in Neural Networks:
- In neural networks, the cost function measures the difference between the predicted and actual values, summed over all training examples.
- To add regularization, an additional term is introduced to the cost function. This term involves a regularization parameter (lambda) and the norm of the weights(w).
- Frobenius Norm: In neural networks, the additional term uses the Frobenius norm, which is the sum of the squares of all elements in the weight matrices. This helps to keep the weights small and prevents the model from overfitting the training data.
- Weight Decay: The regularization term effectively reduces the size of the weights during training, which is why it's sometimes called weight decay. This helps to prevent the network from becoming too complex and overfitting.

![Reg in LR](https://github.com/user-attachments/assets/53919fdd-fc48-45b8-9328-2c1dd75067f2)

### Why does Regularization help with overfitting?
- When a neural network learns not only the underlying patterns in the training data but also the noise, it performs well on training data but poorly on new data.
- Regularization adds a penalty to the cost function to keep the weights small. This helps the model avoid learning the noise and focus on the actual patterns.

### How does it work?
- Intuition: If the regularization term (lambda) is large, the weights (W) are encouraged to be close to zero. This simplifies the model, making it less likely to overfit.
- Cost Function Modification: The cost function is modified to include the regularization term. This new cost function is used in gradient descent.
- Weight Decay: Regularization effectively reduces the size of the weights during training, making the network simpler and less prone to overfitting.

#### Impact on Neural Networks:
- Simplified Network: With large regularization, the weights become small, and the network behaves more like a simpler model (e.g., logistic regression) rather than a complex one.
- Linear Behavior: If weights are small, the activations (outputs of neurons) stay in the linear range of activation functions (like tanh), making the network act almost linearly. This prevents the network from fitting very complex patterns and reduces overfitting.

```
When plotting the cost function during training, use the new cost function (with the regularization term) to see a smooth decrease. If you plot the old cost function (without the regularization term), you might not see this smooth decrease.
```
### Dropout Regularization:
- In this method we randomly eliminate some neurons from each layer in the neural network during the training process.
- So for training each example we randomly eliminate different nodes each time and train the examples on these models.
- By training the network with these smaller, randomly modified versions, dropout helps to regularize the network and prevent overfitting. This ensures that the network learns robust features that are not dependent on any specific neurons.

### Implementation of Dropout Technique:

![Implement Dropout](https://github.com/user-attachments/assets/554366f5-f31c-4d41-a692-5051675c6835)

1. For each training example, randomly disable neurons based on a probability called `keep_prob`.
2. Create a **dropout mask** `D3`, where each element is `1` (keep the neuron) with probability `keep_prob` and `0` (drop the neuron) otherwise.
3. Multiply activations `a3` by `D3` element-wise to apply the dropout.
4. Scale `a3` by dividing by `keep_prob` to maintain the expected value of activations.

```
Inverted Dropout:
The scaling step (a3 /= keep_prob) is known as inverted dropout.
It ensures that the expected value of the activations remains unchanged, simplifying the testing process since no additional scaling is required.
```

### Other Regularization Methods:
- Data Augmentation:
  - In data augmentation we increase the size of a training dataset by, for example: horizontally flipping images, randomly cropping images ,applying small rotations and distortions to images to make additional fake training examples.
  - This helps with the overfitting problem since having additional examples tells the network that even if a dataset is slightly altered or distorted it is still the same example and not a new one.
  - This helps the model generalize better without the need for new independent data.

- Early Stopping:
  - Monitoring the validation error during training and stopping when the error starts to increase, indicating overfitting.
  - Prevents overfitting by stopping training before the model starts to memorize the training data.This ensures that it generalizes well to new, unseen data.
  - Integrates the cost function with regularization, making the training process more complex.


