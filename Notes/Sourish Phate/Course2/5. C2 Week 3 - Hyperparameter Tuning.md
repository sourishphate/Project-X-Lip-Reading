### Hyperparameter:
- Hyperparameters are settings that control the training process and the structure of the neural network.They are not learned directly from the training data but are predefined and adjusted to optimize the model. 
- They control or determine the values of parameters 'w' and 'b'.
- `Learning rate`,which determines how quickly the network learns, is considered to be the most important hyperparameter.
- Others include `Momentum Term`, `Mini-Batch Size` , `Regularization Parameters`,`number of hidden layers` ,`number of hidden units`, `choice of  activation functions` etc.

#### Tuning Process:
- In earlier generations of machine learning algorithms, it was common practice to sample the points in a grid and systematically explore these values. This practice works okay when the number of hyperparameters is relatively small.
- Random Sampling: 
  - Instead of using a grid, choose random points to explore more possible values.
  - The reason you do this is that it's difficult to know in advance which hyperparameters are going to be the most important for your problem. In practice, you might be searching over even more hyperparameters.
- Coarse to Fine Sampling: Start with a broad search over a large space. Then focus on a smaller region where initial results were good, and sample more densely.
- By exploring different hyperparameter values, we can find the hyperparameters that best optimize the network's performance.

### Using Appropriate Scale for Hyperparameter:
- It is important to select the right scale for hyperparameters since this aspect can affect the learning to a large extent.
- If we scaling hyperparameters like Number of hidden units(e.g 50-100) or Number of layers(e.g 2-4) then uniform scaling is a suitable approach.
- But if we are scaling other hyperparameters like learning rate (α) or exponential average (β) i.e parameters with a wide range, then uniform scaling is not the appropriate approach.
- In this case we will have to use logarithmic scaling.

- Example: 
- α from 0.0001 to 1.
   - Uniform sampling would mostly pick values between 0.1 and 1, ignoring the lower end.
   - Logarithmic scale ensures more balanced exploration.
   ```
   R = -4 * np.random.rand()  # Random number between -4 and 0
   α = 10 ** R  # Learning rate between 10^-4 and 1
   ```
- `Hyperparameter values close to 1 require more precise sampling. Small changes near 1 can significantly impact results. So appropriate scaling causes more dense sampling in paces where the hyperparameter is close to its extreme`

![hyper scale](https://github.com/user-attachments/assets/f5599ebb-7f76-406e-9223-6a68fc915113)

#### Approach:
- 1.Babysitting Approach: In this approach we train a single model at a time and then modify the hyperparameters and monitor its performance
- This is suitable if we have limited computational resources.

- 2.Train multiple models: t In this approach we train multiple models in parallel with different hyperparameter settings and compare their performance to choose the best one.
- This is suitable if we have enough computational resources.

![optimize approach](https://github.com/user-attachments/assets/3e3c29b1-4a4e-4f7a-beb2-10e159afe7d1)