### Multi-Class Classification:
- We use Logistic Regression for Binary classification, which gives us one of two possible outputs for given input parameters.This means that the output which is the dependent variable has two possible outcomes.
- Using Softmax Regression we compute predictions such that the outputs for the given inputs could belong to one of multple classes. This is called Multi-Class Classification.

### Softmax Regression:
- The basic idea of softmax regression is to classify inputs into one of C possible classes.
- The output layer consists of C units, where each unit corresponds to one of the classes.Each unit gives us the probability of the image belonging to that category. The probabilities should add up to 1. We use a special activation function called Softmax to calculate these probabilities.
- How Softmax Regression Works:
  - 1. Input to Output Layer:
       - Raw Scores (Z) Calculation:
         - Compute Z for the final layer:
         - `𝑍=𝑊⋅activation of previous layer+𝑏`
         - Where W is the weight matrix and b is the bias vector.

  - 2. Apply Softmax Activation Function:
        - We calculate a temporary variable called `T` by taking the exponential of the values in the output layer.
        - Compute element-wise exponentiation: `T(i) = e^Z(i)`.
        - Then we apply normalization to all exponentiated values T in the output layer such that they add up to 1. This gives us the probabilities for each category.
        - The normalization means the probablity of each T(i) value in output layer which is :
           - `a(i) = e^Z(i)/Σ(e^Z(j))` where j ranges from 1 to C.
           - Where a(i) is the probability for class i and the denominator is the sum of exponentiated scores for all classes.

![sofmax reg](https://github.com/user-attachments/assets/573fc8b5-06f2-4ac5-b759-99b9b4d6742d)

![sofmax eg](https://github.com/user-attachments/assets/c91dedea-32d9-4ac8-9fd5-092bdf378883)


### Training a softmax classifier:
- Softmax vs. Hard Max:
  - Softmax assigns probabilities based on input values.All probabilities sum to 1.
  - Hard Max Strictly assigns a 1 to the category with the highest value and 0 to all other categories.
- How Softmax Classifier is trained:
  - 1. Input to Output Layer:
       - Raw Scores (Z) Calculation:
         - Compute Z for the final layer:
         - `𝑍=𝑊⋅activation of previous layer+𝑏`
         - Where W is the weight matrix and b is the bias vector.

  - 2. Apply Softmax Activation Function:
        - We calculate a temporary variable called `T` by taking the exponential of the values in the output layer.
        - Compute element-wise exponentiation: `T(i) = e^Z(i)`.
        - Then we apply normalization to all exponentiated values T in the output layer such that they add up to 1. This gives us the probabilities for each category.
        - The normalization means the probablity of each T(i) value in output layer which is :
           - `a(i) = e^Z(i)/Σ(e^Z(j))` where j ranges from 1 to C.
           - Where a(i) is the probability for class i and the denominator is the sum of exponentiated scores for all classes. 

  - 3. Define Loss Function:
   - Use the cross-entropy loss function for softmax classification.
     - For a single training example with target class `y` and predicted probabilities `p`, the loss is:
       - `L = -log(p(y))`
     - For the entire training set, the loss function is the average cross-entropy loss over all examples:
       - `J = -1/m Σ (y * log(p))` where `m` is the number of training examples.

  - 4. Compute Gradients:
    - Compute the gradients of the loss function with respect to the weights and biases.
    - The derivative of the cost with respect to `Z` at the last layer is:
     - `∂J/∂Z = Y_hat - Y`
     - Where `Y_hat` is the predicted probabilities and `Y` is the one-hot encoded true labels.

  - 5. Update Parameters:
   - Use gradient descent or a variant (such as SGD, Adam, etc.) to update the weights and biases.
   - The parameters are updated as follows:
     - `W = W - learning_rate * ∂J/∂W`
     - `b = b - learning_rate * ∂J/∂b`

  - 6. Iterate Until Convergence:
    - Repeat steps 1-5 for a number of iterations or until the loss converges to a minimum value.

![softmax loss](https://github.com/user-attachments/assets/5a7c213a-ac0d-4126-baee-7887eaa45e59)









