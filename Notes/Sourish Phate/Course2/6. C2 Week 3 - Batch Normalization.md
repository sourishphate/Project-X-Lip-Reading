### Batch Normalization :
-When training a model, we normalize the input features by subtracting the mean and dividing by the standard deviation.This helps to speed up the learning and help train the network efficienly.
- Batch normalization adjusts the inputs of a neural network layer to maintain mean and variance of the  hidden layer activations within a desirable range. 

- Steps:
- Compute Mean:
  - For a given layer, take all the activations (denoted as Z) in the mini-batch and compute their mean.
- Compute Variance:
  - Calculate the variance of these activations, which measures the spread of the activations around the mean.
- Normalize Activations:
  - Subtract the mean from each activation to center the activations around zero.
  - Divide by the standard deviation (square root of the variance plus a small number ϵ for numerical stability) to scale the activations to have a variance of one.
- Apply Scale and Shift:
  - This step adjusts the normalized activations to have a desirable mean and variance.
  - Multiply the normalized activations by the learnable parameter gamma (γ).
  - Add the learnable parameter beta (β) to shift the activations.This parameter allows the network to adjust the mean of the activations.

![batch norm1](https://github.com/user-attachments/assets/0ce7c663-3394-4dfb-93c0-0e9e099b2d30)

![batch norm](https://github.com/user-attachments/assets/30f50ffa-71e7-4449-a1b1-00d972e576dd)

### Fitting batch normalization into neural network:
-  Batch Normalization
  - Apply Batch normalization to the activations as the steps given above.
  - Then apply an activation function like Sigmoid , ReLU etc to the normalized activations.
- Forward Propagation through Network
  - Pass the activated values through each layer of the network.
  - Apply Batch Normalization at each layer to ensure that the inputs maintain a consistent mean and variance.
- Backpropagation
  - Compute gradients of the loss with respect to the weights  W and Batch Norm parameters γ and β.
  - No need to calculate gradients for the bias term B , as it is neutralized during normalization.
- Update Parameters
  - Update the weights W and the Batch Normalization parameters γ and β using their respective gradients.
  - This process refines the model based on the computed gradients.
- Training Process
  - Utilize optimization algorithms like gradient descent or Adam to adjust the weights W and parameters γ and β based on the gradients.
  - This iterative adjustment helps minimize the loss function.

![fit batch norm 1](https://github.com/user-attachments/assets/dc473fa7-1523-4e84-b1a5-e509b6d34966)

![fit batch norm 2](https://github.com/user-attachments/assets/6ae17469-db77-4fa0-a2e1-d29765956237)

#### Implementing Gradient Descent using Batch Normalization:
![batch norm 3](https://github.com/user-attachments/assets/16d77e54-a098-4a67-930d-a1745e7ed3b7)

### Why does Batch Normalization Work?
-  Normalizing Input Features:
  - Batch Norm normalizes input features to a similar range, speeding up learning by making features have consistent scales. It also normalizes values in hidden layers.

- Reducing Covariate Shift:
  - It stabilizes learning by controlling changes in the distribution of hidden layer values. This means that updates in earlier layers don’t drastically affect later layers.

- Improving Robustness:
  - Batch Norm helps the network perform better on new data distributions, such as colored cats when trained on black cats, by reducing the impact of input distribution changes.

- Regularization Effect:
  - It adds slight noise to the learning process, similar to dropout, which helps prevent overfitting. However, this effect is minor compared to dropout and decreases with larger mini-batch sizes.

- Training vs. Testing:
  - During training, Batch Norm uses mini-batch statistics. At test time, use running averages of mean and variance from training to normalize data for consistent predictions.

![why Bn work](https://github.com/user-attachments/assets/c77c5fda-b59c-4557-a0d3-306b2f90e554)

![bn wotk 2](https://github.com/user-attachments/assets/b80612ec-4dd7-4495-b8e0-35b45e4e3797)

### Batch Normalization During Testing:
- During training, compute the mean and variance of the mini-batch. Normalize the values using these statistics and scale them with gamma and beta.
- When processing single examples at test time, compute the mean and variance from training. Use an exponentially weighted average to estimate these statistics.
- Keep track of mean and variance from each mini-batch during training. Use these running averages to normalize data at test time.
- Apply the computed mean and variance from training to normalize test data. This ensures consistency even with single examples.
- Deep learning frameworks usually handle the estimation of mean and variance automatically. Any reasonable approach to estimating these values should work well.

![bn test time](https://github.com/user-attachments/assets/59fa8bf2-7a65-42ac-8253-ace032084f30)