## Deep Neural Networks

- A deep neural network is a type of machine learning model that is more complex than a simple logistic regression or a neural network with just one hidden layer.
- It consists of multiple hidden layers, which makes it a "deep" model.

### 1. Forward Propagation

- Forward propagation is the process of passing input data through the neural network to compute the output.

### 2. Matrix Dimension

- The dimensions of the weight matrix (W) for each layer depend on the number of units in the current layer and the previous layer.

```
For example,
The input layer has 2 features, and the hidden layers have 3, 5, 4, and 2 units respectively. The output layer has 1 unit.
```

_The weight matrix for the first hidden layer (Layer 1) will be 3x2, because it needs to transform a 2-dimensional input into a 3-dimensional output._

![intuition](https://github.com/user-attachments/assets/fe545970-714f-4597-b79d-ecf385ce60c7)

### 3. Building Block of Deep Neural Netrwork

- To build a deep neural network, we need to understand two important steps: forward propagation and back propagation.
- Forward propagation is the process of taking the input data and passing it through the network to get the output.
  - Each layer in the network performs a computation using the parameters (weights and biases) and the activations from the previous layer.
  - The output of each layer becomes the input for the next layer until we get the final output.
- Back propagation is the process of calculating the gradients or derivatives of the parameters in the network.
  - These gradients tell us how much each parameter needs to be adjusted to minimize the error between the predicted output and the actual output.
  - This adjustment is done using a technique called gradient descent.

![forward and back prop](https://github.com/user-attachments/assets/8ca3827f-2643-4d56-a5a6-2dd7411afbb2)

#### 3.1 Forward propagation for layer l

![forward layer l](https://github.com/user-attachments/assets/b41e4843-6e54-4e5e-a9f0-b1735cadfd6f)

#### 3.2 Backward propagation for layer l

![back prop layer](https://github.com/user-attachments/assets/cd2553fd-776e-4bab-a0e2-9febef36b034)

### 4. Parameters vs Hyperparameters

- Hyperparameters in deep learning are settings that control how a model learns and performs.
- These settings include the learning rate, number of hidden layers, number of hidden units, and choice of activation function.
- Hyperparameters are different from the parameters of the model (weights and biases) as they determine the values of these parameters.

![last](https://github.com/user-attachments/assets/df7217be-306e-421d-b53f-012544608c34)
