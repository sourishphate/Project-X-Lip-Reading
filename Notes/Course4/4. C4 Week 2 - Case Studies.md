### Classic neural network architectures:
- 1. LeNet-5:
  - Purpose: Recognize handwritten digits.
  - Input: 32x32 grayscale images.
  - Architecture:
    - Convolution Layer: Applies 6 filters (5x5) to the image, producing 6 feature maps of size 28x28.
    - Average Pooling Layer: Reduces each feature map to 14x14 by averaging values in 2x2 blocks.
    - Second Convolution Layer: Uses 16 filters (5x5) to produce 16 feature maps of size 10x10.
    - Second Pooling Layer: Reduces each feature map to 5x5 by averaging in 2x2 blocks.
     - Fully Connected Layers: Flattens the 5x5x16 maps into a vector of 400 nodes, connected to 120, then 84 neurons, and finally to 10 output neurons for digit classification.

![lenet5](https://github.com/user-attachments/assets/de1330e6-0d7d-4e22-9caa-ed12a72a9288)

- 2. AlexNet:
  - Purpose: Classify general images.
  - Input: 227x227 RGB images.
  - Architecture:
    - Convolution Layer: Uses 96 filters (11x11) with a stride of 4, resulting in 96 feature maps of size 55x55.
    - Max Pooling Layer: Reduces each feature map to 27x27 by selecting the maximum value in 3x3 blocks.
    - Second Convolution Layer: Applies 256 filters (5x5), producing 256 feature maps of size 13x13.
    - Second Pooling Layer: Reduces each feature map to 6x6.
    - Fully Connected Layers: Flattens the 6x6x256 maps into 9216 nodes, connected to two layers of 4096 neurons, and finally to a softmax output for 1000 categories.

![alexnet](https://github.com/user-attachments/assets/0e0ac2f1-abd8-48c8-9b8b-c0190bcc980b)

- 3. VGG-16:
  - Purpose: Simplified image classification.
  - Input: 224x224 RGB images.
  - Architecture:
    - Convolution Layers: Uses layers with 64 filters (3x3), producing 64 feature maps of size 224x224.
    - Max Pooling Layer: Reduces size to 112x112 by taking the maximum value in 2x2 blocks.
    - Additional Convolution Layers: Increases filters to 128, 256, and 512 in subsequent layers, each followed by pooling that halves the dimensions.
    - Fully Connected Layers: Flattens the final feature maps to a vector, connects to two layers of 4096 neurons, and ends with a softmax output for 1000 categories.

![vgg](https://github.com/user-attachments/assets/05ed68b1-d0a0-404a-880e-bfc5cbdb4b22)


### ResNets:
- In tradtional networks the data flows through multiple layers sequentially. For example, if you start with activation a(L) , it moves through layers, becoming a(L+1) and a(L+2) after several computations.
- Residual Networks introduce skip connections or shortcuts. These connections bypass one or more layers, allowing the original activation a(L) to be added directly to the output of deeper layer activations without having to pass throgh each layer to that point in the network.

- Example:
  - Traditional Layers: `a(L+2) = g(Z(L+2))` where g is a non-linearity applied to the output of a layer.
  - Residual Block: `a (L+2 ) = g(Z(L+2)+ a(L))`.  Here, a(L) is added directly to the output of the deeper layers before applying the non-linearity.

![resnet1](https://github.com/user-attachments/assets/289ae895-2212-4819-8f60-9c371ddb4d3d)

- Building ResNets:
  - Original Path: The activation a(L) is processed through several layers:
       - Linear Transformation: Multiply by weight matrix and add bias.
       - Non-linearity: Apply a function like ReLU.
  - Residual Path:
    - Residual Block: Stack multiple residual blocks together. Each block includes a skip connection that adds the original activation to the deeper layer’s output.
    - `Skip Connection is the process where the original activation is added to the result in a much deeper layers (without passing through each layer sequentially) and non-linearity are applied.`
    - Network Depth: ResNets can have hundreds or even thousands of layers, as these skip connections help deal with issues like vanishing gradient that arise in deeper networks.

![resnet2](https://github.com/user-attachments/assets/19ba4b05-c0d9-44eb-a5de-27928a240e28)

#### Why ResNets Work:
- ResNets use skip connections (shortcuts) that add the input directly to the output of deeper layers. This makes it easy for the network to learn that some layers might just pass the input unchanged, which helps avoid training problems.
- The skip connections allow the network to effectively skip layers if they’re not useful, making it easier to train even very deep networks.
- To use skip connections, the dimensions of the input and output must match. If they don’t, extra operations like 1x1 convolutions adjust the dimensions to match.
- If the weights of the added layers are zero, the output a(L+2) will be equal to a(L). This means adding extra layers does not hurt the network’s performance but might even help.
