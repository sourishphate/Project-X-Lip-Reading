### Classic neural network architectures:
- 1. LeNet-5:
  - Purpose: Recognize handwritten digits.
  - Input: 32x32 grayscale images.
  - Architecture:
    - Convolution Layer: Applies 6 filters (5x5) to the image, producing 6 feature maps of size 28x28.
    - Average Pooling Layer: Reduces each feature map to 14x14 by averaging values in 2x2 blocks.
    - Second Convolution Layer: Uses 16 filters (5x5) to produce 16 feature maps of size 10x10.
    - Second Pooling Layer: Reduces each feature map to 5x5 by averaging in 2x2 blocks.
     - Fully Connected Layers: Flattens the 5x5x16 maps into a vector of 400 nodes, connected to 120, then 84 neurons, and finally to 10 output neurons for digit classification.

![lenet5](https://github.com/user-attachments/assets/de1330e6-0d7d-4e22-9caa-ed12a72a9288)

- 2. AlexNet:
  - Purpose: Classify general images.
  - Input: 227x227 RGB images.
  - Architecture:
    - Convolution Layer: Uses 96 filters (11x11) with a stride of 4, resulting in 96 feature maps of size 55x55.
    - Max Pooling Layer: Reduces each feature map to 27x27 by selecting the maximum value in 3x3 blocks.
    - Second Convolution Layer: Applies 256 filters (5x5), producing 256 feature maps of size 13x13.
    - Second Pooling Layer: Reduces each feature map to 6x6.
    - Fully Connected Layers: Flattens the 6x6x256 maps into 9216 nodes, connected to two layers of 4096 neurons, and finally to a softmax output for 1000 categories.

![alexnet](https://github.com/user-attachments/assets/0e0ac2f1-abd8-48c8-9b8b-c0190bcc980b)

- 3. VGG-16:
  - Purpose: Simplified image classification.
  - Input: 224x224 RGB images.
  - Architecture:
    - Convolution Layers: Uses layers with 64 filters (3x3), producing 64 feature maps of size 224x224.
    - Max Pooling Layer: Reduces size to 112x112 by taking the maximum value in 2x2 blocks.
    - Additional Convolution Layers: Increases filters to 128, 256, and 512 in subsequent layers, each followed by pooling that halves the dimensions.
    - Fully Connected Layers: Flattens the final feature maps to a vector, connects to two layers of 4096 neurons, and ends with a softmax output for 1000 categories.

![vgg](https://github.com/user-attachments/assets/05ed68b1-d0a0-404a-880e-bfc5cbdb4b22)


### ResNets:
- In tradtional networks the data flows through multiple layers sequentially. For example, if you start with activation a(L) , it moves through layers, becoming a(L+1) and a(L+2) after several computations.
- Residual Networks introduce skip connections or shortcuts. These connections bypass one or more layers, allowing the original activation a(L) to be added directly to the output of deeper layer activations without having to pass throgh each layer to that point in the network.

- Example:
  - Traditional Layers: `a(L+2) = g(Z(L+2))` where g is a non-linearity applied to the output of a layer.
  - Residual Block: `a (L+2 ) = g(Z(L+2)+ a(L))`.  Here, a(L) is added directly to the output of the deeper layers before applying the non-linearity.

![resnet1](https://github.com/user-attachments/assets/289ae895-2212-4819-8f60-9c371ddb4d3d)

- Building ResNets:
  - Original Path: The activation a(L) is processed through several layers:
       - Linear Transformation: Multiply by weight matrix and add bias.
       - Non-linearity: Apply a function like ReLU.
  - Residual Path:
    - Residual Block: Stack multiple residual blocks together. Each block includes a skip connection that adds the original activation to the deeper layer’s output.
    - `Skip Connection is the process where the original activation is added to the result in a much deeper layers (without passing through each layer sequentially) and non-linearity are applied.`
    - Network Depth: ResNets can have hundreds or even thousands of layers, as these skip connections help deal with issues like vanishing gradient that arise in deeper networks.

![resnet2](https://github.com/user-attachments/assets/19ba4b05-c0d9-44eb-a5de-27928a240e28)

#### Why ResNets Work:
- ResNets use skip connections (shortcuts) that add the input directly to the output of deeper layers. This makes it easy for the network to learn that some layers might just pass the input unchanged, which helps avoid training problems.
- The skip connections allow the network to effectively skip layers if they’re not useful, making it easier to train even very deep networks.
- To use skip connections, the dimensions of the input and output must match. If they don’t, extra operations like 1x1 convolutions adjust the dimensions to match.
- If the weights of the added layers are zero, the output a(L+2) will be equal to a(L). This means adding extra layers does not hurt the network’s performance but might even help.
- The key idea behind ResNets is that they make it easy for the network to learn the identity function, which is why adding extra layers doesn't harm the network's performance.

### Network in Network - 1*1 Convolution:
- A 1x1 convolution is a convolution operation where the filter size is 1x1. This means that the filter only covers a single pixel at a time but spans across all the channels of the input volume.
- If you apply a 1x1 convolution with a single filter, it's equivalent to multiplying each pixel by a constant value (the filter weight). This is essentially a scaling operation.
- Each 1x1 filter can be thought of as a neuron that takes as input all the channel values at a given pixel location and outputs a single value.
- It is capable of shrinking channels.

- Multi-Channel Input:
  - When dealing with an image with multiple channels (e.g., 6x6x32):
    - Each position in the 6x6 grid now has 32 values (one for each channel).
    - A 1x1 filter will still cover a single pixel spatially, but it will operate across all 32 channels.

- Multiple Filters:
  - If you have multiple 1x1 filters, each filter will produce a separate output channel. For example:
  - With 32 filters, the output volume will be 6x6x32.
  - Each filter will produce one channel of the output by combining all input channels at each spatial location.

![1x1](https://github.com/user-attachments/assets/85aed829-897f-4756-9007-8d81516aadcf)

- How It Works:
  - Element-wise Multiplication: The 1x1 filter will multiply each of the 32 values at a specific position by its corresponding weight.
  - Summation: The results of these multiplications are summed up to produce a single value.
  - Non-linearity: A non-linear activation function (like ReLU) is applied to this sum.
  
![use 1x1](https://github.com/user-attachments/assets/3b0dd306-9ec2-4c0d-9bbd-a2673925e7fc)

### Inception Network:
- When designing a layer for a convolutional network, we need to choose between different filter sizes (e.g., 1x1, 3x3, or 5x5) or even a pooling layer.
- The Inception Network addresses this by using all of them, resulting in a more complex but remarkably effective network architecture.
- This involves applying several different filters and concatenating the outputs of each filter to get the final output.
- Problem:
  - When using a 5x5 filter in the Inception module, the number of multiplications and additions needed to compute the output can be quite high.
  - This can lead to longer training times and increased resource requirements, making the network slower and more computationally expensive.
- Solution:
  - To deal with this problem we use 1x1 convolutions to reduce the number of channels before applying larger filters.
  - 1x1 convolutions act as bottlenecks to reduce the computational cost without losing performance.

![inc networks](https://github.com/user-attachments/assets/d90b2b55-a796-441e-90ee-4517fa6c2b01)

- Inception Module
  - Input from Previous Layer:
   - Takes the output from a previous layer as input.
   - Applies different types of convolutions to capture various details.

  - Variety of Convolutions:
   - Uses convolutions with different filter sizes and depths.
   - Captures multiple levels of detail in the image.
  
  - Concatenation:
   - Combines the outputs of these convolutions into a single output.
   - Enables the network to capture a wide range of features at different scales and resolutions.
  
- Inception Network
    - Repeated Blocks:
     - Consists of multiple repeated blocks, each containing several Inception modules.
      - Stacked together to create a deep network capable of learning complex patterns.
   
    - Side-Branches:
      - Includes side-branches that take hidden layers and use them to make predictions.
      - Helps regularize the network and prevent overfitting.

![inc net](https://github.com/user-attachments/assets/1f39a36b-a573-4dc4-9d60-d56be75e5213)

### MobileNet:
- MobileNets is a foundational CNN architecture designed for efficient performance on low-power devices, like mobile phones.
- MobileNets leverage depthwise separable convolutions to reduce computational cost significantly.
- Normal v/s depthwise seperable convolution
  - Normal convolution:In a normal convolution, you have an input image and a filter. The filter is moved across the image, performing multiplications and additions to produce an output. This process can be computationally expensive.
  - Depthwise separable convolution:Instead of using one filter for all the channels in the input image, MobileNets use separate filters for each channel.This reduces the number of multiplications needed. Then, a pointwise convolution is applied to combine the outputs of the separate filters.

![depth conv](https://github.com/user-attachments/assets/edb087ce-e436-4e29-8e6c-db30a32f4cd0)

- Depthwise Separable Convolution:
  - This approach reduces computations by separating the convolution into two steps:
    - 1. Depthwise Convolution: 
        - **Example Input:** `6 × 6 × 3`  
        - **Filter:** `f × f` (e.g., `3 × 3`)  
        - **Number of Filters:** Equal to the number of input channels (`n_c`)
        - In depthwise convolution, each filter operates only on its corresponding input channel. For each position of the filter, you perform a set number of multiplications.
        -  The total number of computations is calculated by multiplying the number of filter positions by the filter size and the number of input channels.

![depht conv1](https://github.com/user-attachments/assets/4707bcbe-c15b-443f-b06a-4a95bcd03c10)

    - 2. Pointwise Convolution: Use 1x1 convolutions to combine the output of the depthwise step.
        - **Input from Depthwise Step:** `4 × 4 × 3`  
        - **Filter:** `1 × 1 × 3`  
        - **Number of Filters:** `n_c'` (e.g., `5`)
        - Pointwise convolution combines the outputs from the depthwise step using `1 × 1` filters.
        -  Each filter position requires a few multiplications, and the total number of computations is the product of the number of filter positions, the filter size, and the number of filters.

![depth conv2](https://github.com/user-attachments/assets/fb6e8f04-4bc6-4852-89f5-73a19c33b962)

### MobileNet Architecture:
- 1. MobileNet v1:
    - It consists of multiple blocks, each using depthwise separable convolution. 
    - The architecture includes `13 such layers`, followed by pooling, a fully connected layer, and a softmax for classification. 
    - This approach significantly reduces computational cost compared to traditional convolutions.

![mob v1](https://github.com/user-attachments/assets/85f3c842-b327-423b-813e-a5934cd83765)

- 1. MobileNet v2 Architecture:
    - Residual Connections: Similar to ResNet, MobileNet v2 adds residual (skip) connections. These connections pass the input from a previous layer directly to the next, helping gradients propagate more efficiently.
    - Bottleneck Blocks: MobileNet v2 incorporates a bottleneck block with an expansion layer. The block functions as follows:
      - Expansion: We apply multiple 1x1 convolutions to increase the dimensions of the output, e.g., from 6x6x3 to 6x6x18.
      - Depthwise Separable Convolution: Applies depthwise and pointwise convolutions to transform the expanded features while maintaining the output dimensions by using padding.
      - Projection: Uses a 1x1 convolution to reduce the feature dimensions back to the original, e.g., from 6x6x18 to 6x6x3.
    `MobileNet v2 repeats the bottleneck block 17 times, followed by pooling, a fully connected layer, and a softmax for classification. `

![mob v2](https://github.com/user-attachments/assets/c1db0ba3-e0a7-45a1-9f6b-b0630813ee77)

### EfficientNet:
- EfficientNet provides a way to automatically scale networks based on available computational resources. 
- This is useful when working with devices of varying compute power, like different mobile phones or edge devices.
![efficient net](https://github.com/user-attachments/assets/93a625ce-472c-49d8-8c18-490b365f5eaf)

### Practical advice on using ConvNet:
#### Transfer learning:
- Transfer learning involves using a pre-trained neural network—one that has been trained on a large dataset—to help with a new, often smaller dataset.
- Instead of training a model from scratch, we leverage the learned features and weights from a model that has already been trained on a similar problem.
- It saves us time and computational resources by using pre-trained networks as a starting point for our own specific tasks.

![transfer learn](https://github.com/user-attachments/assets/e7b44903-cbcf-41c0-a86b-b07b2ed38655)

#### Data augmentation:
- Data augmentation is used in computer vision to enhance model performance, especially when large amounts of data are not available.
- It helps by creating variations of the existing data to improve the model.
- Common Data Augmentation Techniques:
   - Mirroring: This technique is used to flip images horizontally.Effective if mirroring preserves the object’s identity (e.g., flipping a cat image still shows a cat).
   - Random Cropping: Randomly crop different parts of the image. Helps the model learn features from various image regions.
   - Color Shifting: Used to adjust the intensity of color channels (RGB).Makes the model robust to color variations due to lighting changes.It involves changing the colors of the image by adding or subtracting values from the red green and blue channels.

![augment1](https://github.com/user-attachments/assets/97e540cd-7b53-492d-b38f-e3ced340795c)

![augment2](https://github.com/user-attachments/assets/6eb66a30-99b8-4208-ae3e-22f7ed734b4a)

