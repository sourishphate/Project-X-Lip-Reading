### Object Localization:
- Object detection is a rapidly advancing area in computer vision, enabling systems to identify and locate multiple objects within an image.
- Image Classification : In image classification, a neural network processes an image and outputs a class label (e.g -  a "car").
-Classification with localization extends image classification by not only identifying the object but also determining its position within the image using a bounding box. 
- Understanding localization helps in handling more complex object detection tasks.

![class w local](https://github.com/user-attachments/assets/ecd3f401-663a-4c5b-9170-db2666d04235)

- Parameters:
  - B_x: x-coordinate of the bounding box center
  - B_y: y-coordinate of the bounding box center
  - B_h: height of the bounding box
  - B_w: width of the bounding box

- Training the Network:
  - Neural network outputs:
    - P_c: Probability of an object.
    - B_x, B_y, B_h, B_w: Bounding box dimensions.
    - C_1, C_2, C_3 etc. : Class labels (e.g., pedestrian, car, motorcycle).

- Target Label (Y):
  - With object: [P_c, B_x, B_y, B_h, B_w, C_1, C_2, C_3]
  - Without object: [P_c, ?, ?, ?, ?, ?, ?, ?] ('?' means "don't care" when P_c is 0 i.e object probability is 0)
 
- Loss Function:
  - If object is present i.e P_c = 1 then we minimize the loss function for all components present in the output.
  - If object is not present i.e P_c = 0 then we minimize the loss function only for P_c

![classw local 2](https://github.com/user-attachments/assets/435519e6-07b4-4cd8-ba6c-b37bd9ec212f)

### Landmark Detection:
- Landmark detection involves training a neural network to output the coordinates of important points called landmarks in an image. 
-  These specific points in an image are important for identifying features or parts of objects.
- We use a neural network that is trained to output the coordinates of these landmarks.
- By training the neural network with labeled data that contains the coordinates of these landmarks, we can teach it to recognize and locate them accurately.
-Training the Network:
  - A labeled dataset is required where each image has specific points (landmarks) marked with their x and y coordinates.
  - Manual annotation is often required to create the training data.
  - Feed the image into a convolutional neural network (CNN) or similar architecture.
  - The network outputs the x and y coordinates for each landmark.
  - Use loss function to minimize error between loss actual and predictd outputs .

`This method has many practical appilcations such as Face Recognition, Emotion Recognition, Pose Detection, applying augmented reality (AR) effects etc.`

![landmark detect](https://github.com/user-attachments/assets/76fb8264-a35b-445a-86f4-b36851dbe378)

## Object Detection:

###  1. Sliding Windows Detection:
- Create a Training Set: This training will conatain images with and without the object of interest.
- Train a Classifier|: Use a classifier to learn to distinguish between images with and without the object. This classifier will later predict whether a given window contains the object.
- Sliding Windows on Image: 
   - Move a small window across the image and check each region for the object.
   - Repeat with bigger windows to find larger objects.
   - Move the window in steps to cover the whole image.
- Detection: If any window outputs a positive prediction (indicating the presence of the object), record the position.
- Challenges: Sliding windows can be slow because you need to process many windows at different positions and sizes. Smaller strides give better results but are more computationally expensive.Larger stride reduces the number of windows but can miss objects.

![sliding window](https://github.com/user-attachments/assets/6d2d9aa5-b9ec-486f-9c4b-9774f62dad95)

#### 1.1 Convolutional Implementation Sliding Windows:
- Original Method: Sliding Windows:
  - Sliding Window Technique:
    - You take small chunks (like 14x14 pixels) of an image.
    - For each chunk, you run it through the network to classify what’s in that chunk.
    - This process is repeated for many overlapping chunks of the image.
    - Problem: It’s slow because you run the network multiple times for each chunk.

![sliding conv1](https://github.com/user-attachments/assets/e6813aea-7598-48d4-9a36-cb9700360112)

- Sliding Window using Convolutional Layers:
  - Initial Setup:
    - Input Image Size: Let's say it's 16x16x3.
    - Sliding Window Size: 14x14 (the window size used for object detection).
  - Apply Convolutional Layers:
    - First Layer: The network processes the whole 16x16x3 image using convolutional layers to create feature maps.
    - Example Output Size After Convolution: 12x12x16 (if using 5x5 filters with padding).
  - Pooling and Further Convolutions: 
    - Apply max pooling to reduce dimensions.
    - Additional convolutional layers reduce the feature map size further.
  - Final Layer:
    - Fully Connected Layers: After several convolutional and pooling layers, the network ends with a 1x1x400 volume.
    - 1x1 Convolutional Layer: This transforms the 1x1x400 volume into a 1x1x4 volume, indicating probabilities for 4 classes.
    - Finally, it uses a softmax activation function to output a 1 by 1 by 4 volume, representing the probabilities of different object classes.

- Sliding the Window:
  - Instead of manually cropping out each 14x14 region and passing it through the network, the entire image is processed at once.
  - The network’s output includes the class probabilities for each possible 14x14 region in the image.
  -  Convolutions share computations across overlapping regions of the image, making it much faster and efficient.

`The convolutional implementation of sliding windows allows the algorithm to share computation and make predictions for the entire image at once, instead of running the algorithm multiple times for different regions of the image.`

![sliding conv2](https://github.com/user-attachments/assets/7f38d64d-0991-4ab8-bd97-82d3f9e53353)

#### 1.2 Bounding box predictions:
##### YOLO (You Only Look Once) algorithm:
- The YOLO (You Only Look Once) algorithm significantly improves the accuracy and efficiency of object detection compared to earlier sliding window methods.
- Grid Division:
  - The input image is divided into a grid.
  - For example,we consider a 3x3 grid though in practice we often uses grids like 19x19.
- Grid Cells
  - Each cell in the grid is responsible for detecting objects that fall within its boundaries.
- Apply Classification and Localization:
   - For each grid cell, apply an image classification and localization algorithm. This helps determine if there’s an object within that grid cell and predicts the object's class and bounding box.

- Create Labels for Each Grid Cell:
   - Each grid cell generates a label vector. This vector includes:
     - `P_c`: Probability of an object being present.
     - `B_x`, `B_y`: Coordinates of the bounding box's center relative to the grid cell.
     - `B_h`, `B_w`: Height and width of the bounding box relative to the grid cell.
     - `C_1`, `C_2`,` C_3`: Class probabilities for different object categories (e.g., car, pedestrian).

- Train the Neural Network:
   - Use input images and the corresponding label vectors to train a neural network. The network learns to map input images to an output volume, which contains the predicted bounding boxes and class probabilities for each grid cell.

- Make Predictions at Test Time:
   - Feed an input image into the trained neural network. The network outputs the presence of objects, their class labels, and bounding box coordinates for each grid cell.

![yolo](https://github.com/user-attachments/assets/11786264-8135-4daf-9b1a-b5481589aa74)

### Intersection over union:
- Intersection Over Union (IoU) is a metric used to evaluate the performance of an object detection algorithm by comparing the predicted bounding box to the actual bounding box that is seen in the output.
- Bounding Boxes:
  - Ground Truth Bounding Box: The actual location of the object in the image.
  - Predicted Bounding Box: The location predicted by your object detection algorithm.
- The intersection is the area where the predicted bounding box overlaps with the ground truth bounding box. This is the shaded region where both boxes cover the same space.
- Union is total area covered by both bounding boxes combined. This is the area covered by either of the boxes or both.
- `IoU Formula: Area of Union/Area of Intersection`

` IoU measures how similar two bounding boxes are. This helps in assessing the performance of object detection models.`
```
IoU = Intersection Area / Union Area
IoU > 1 -> Perfect
IoU > 0.5 -> Acceptable
IoU = 0 -> No overlap.
```

![iou](https://github.com/user-attachments/assets/4467fbc0-8895-4fa7-b164-672f0d169a68)

### Non-Max Suppression:
- One of the problems of object detection is that our algorithm may find multiple detections of the same object.Non-Max Suppression helps us tackle this problem i.e our algorithm deects the object only once.
- Multiple grid cells might detect the same object, leading to several overlapping bounding boxes for the same object.
- NMS Algorithm:
  - Remove bounding boxes with a probability lower than a certain threshold for proablity of object being present(e.g.P_c< 0.6).`P_c`is robability of an object being present.
  - Choose the bounding box with the highest probability (e.g., Box 1 with 0.9).
  - Compare the chosen box with the remaining boxes and remove (suppress) boxes that overlap significantly with the chosen box based on Intersection Over Union (IoU).For example if a box has IoU greater than 0.6 with the chosen box then we discard it.
  - Continue the process with the next highest probability box, and repeat until all boxes are processed.
- Only keep the boxes that are not suppressed, resulting in a cleaner set of detections.

![nms](https://github.com/user-attachments/assets/fbc96454-298e-4ed8-b9b9-43dfb4de0575)

### Anchor Boxes:
- One of the challenges in object detection is handling cases where multiple objects can overlap or be very close to each other, making it hard for the computer to tell them apart.. Anchor boxes provide a solution by allowing multiple detections per grid cell.
- To address this, we use predefined shapes called anchor boxes. 

![anchor 1](https://github.com/user-attachments/assets/1ca5d987-3619-45a9-bbaa-765a01ba7a18)

- How it Works:
  - Each grid cell outputs a vector for each anchor box. For two anchor boxes, the output vector looks like this:
` [PC1, BX1, BY1, BH1, BW1, C1_1, C1_2, C1_3, PC2, BX2, BY2, BH2, BW2, C2_1, C2_2, C2_3]`
  - But instead of just detecting one object per cell, it now associates each object with the anchor box that best matches its shape.
  - When the computer analyzes the image, it assigns each object to the grid cell that contains its center point.
  
![anchor2](https://github.com/user-attachments/assets/00146676-bb74-43e8-8155-a8a883796178)

### YOLO:
![yolo1](https://github.com/user-attachments/assets/913e79d1-0f52-490d-b147-8c41c49dc619)

![yolo2](https://github.com/user-attachments/assets/c01575af-619b-4fc0-9997-28ecf0acf31f)

