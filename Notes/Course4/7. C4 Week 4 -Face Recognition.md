###  One-Shot Learning in Face Recognition:
- One-shot learning is a technique in face recognition, where the goal is to recognize a person given only one image.
- Example:
  - We have a database with a single image of each employee. When a new image is presented, the system must recognize if this is the same person as in the database or a new person not in the database.

- Traditional Approach
  - Method: Use a Convolutional Neural Network (ConvNet) to classify the input image.
  - Issue: This requires a large dataset for training. Additionally, adding new people requires retraining the entire network, which is impractical.

- Similarity Function:
  - Instead of classifying, learn a similarity function d that compares two images and outputs a measure of difference.

![one shot](https://github.com/user-attachments/assets/3b22d860-27b0-4e1a-b702-6f2b8be46fc4)

- How it Works:
  - Learning the Function `d`:
    - We input two images and output the degree of difference `d` between the two images.
    - For images of the same person, d should be small. For different people, d should be large.
  - Recognition Process:
    - For a new image, compare it with each image in the database using d.
    - If the difference is less than a threshold `τ`, the images are of the same object.
`Adding a new person to the database does not require retraining the network. Simply add the new image and use d for comparisons.`

![one shot 2](https://github.com/user-attachments/assets/4f2e7cbf-5bd6-4e0d-9ac7-bd7aca4492f5)

### Siamese Network:
- Architecture:
  - The Siamese network consists of two identical convolutional neural networks that take in two different input images and produce encodings for each image.
  - These encodings are vectors of 128 numbers that represent the input images.
  - `The goal is to train the network so that the distance between the encodings of two images is small if the images are of the same person, and large if the images are of different people. `
  - Function `d` Measures the distance between the two feature vectors.

![siamese1](https://github.com/user-attachments/assets/1173599a-d6ff-44ed-b80c-7c7baa2e3bd3)

- Training :
  - Train the network so that the distance `d` is small for images of the same person and large for images of different people.
  - The goal is to learn the parameters such that if 2 images `x_i` and `x_j` are similar then the difference `d` between the encodings(featue vectors) of the two images is very small and large if they are different.
  - `Adjust the parameters of the neural network using backpropagation to minimize the distance between encodings of similar images and maximize the distance between encodings of different images.`

  - Logistic Regression:
     - We use logistic regression on the encodings to predict if two images are of the same person or different people.
     - The logistic regression unit uses the absolute differences of the encodings as features.
     - The logistic regression output y_hat is a sigmoid function applied to these features, where: 
       - `y_hat = 1` if the images are of the same person 
       - `y_hat = 0 ` if they are different.
     - Adjust the parameters of the logistic regression unit during training to optimize the classification accuracy.

![siamese2](https://github.com/user-attachments/assets/9ff516cf-76fe-4717-a74e-d4354bf15b83)

![learn siam](https://github.com/user-attachments/assets/19d34909-5c58-4b49-805e-ba1d5293ad01)

### Triplet loss:
- Triplet loss is used to train neural networks to create good encodings for images, particularly in face recognition.
- `It works by comparing three images at a time: an anchor image (A), a positive image (P) of the same person as the anchor, and a negative image (N) of a different person.`
- The goal is to ensure that the distance `d` between the anchor and the positive image is smaller than the distance between the anchor and the negative image by at least a margin `α`.

```
Margin (α) is important because it Prevents trivial solutions where all encodings are the same or zero.It also ensures that the distance between different persons (anchor-negative) is significantly larger than the distance between the same person (anchor-positive).
```

- Mathematical Formula:
  - Distance function d:
    - d(A, P)≤ d(A, N) - α
    - d(A, P) - d(A, N) ≤ α
  - Triplet Loss Function:
    - L(A, P, N) = max(0, d(A, P) - d(A, N) + α)

![triplet 1](https://github.com/user-attachments/assets/b12e10c8-e8e6-46b5-8022-384aa4b65b7f)

![triplet 2](https://github.com/user-attachments/assets/cdc7343e-d704-462c-b446-b7119d134ac8)

- Choosing Triplets for Training:
  - Random selection of triplets can lead to easy examples that don't contribute much to learning.
  - Hard triplets force the network to learn more robust features to distinguish between similar faces.
  - Hard triplets are those where d(A,P) is close to d(A,N). These are more useful for effective learning.

![triplet 3](https://github.com/user-attachments/assets/44fd7fc7-c74f-4e02-a7a9-aa98bb28555e)

- Training :
  - Dataset:
    - Multiple images per person to form anchor-positive pairs.
    - A large, diverse dataset for effective encodings.
  - Implement:
    - Generate triplets (A, P, N) from the dataset.
    - Compute the triplet loss for each triplet.
    - Use gradient descent and back propagation to minimize the total loss over the entire dataset.
