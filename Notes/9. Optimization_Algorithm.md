### Batch v/s mini-batch gradient descent

- To speed up the training process, we can use a technique called mini-batch gradient descent.
- Instead of processing the entire training set at once, we split it into smaller subsets called mini-batches.
- Each mini-batch contains a smaller number of examples, making it faster to process.

![batch vs mini](https://github.com/user-attachments/assets/7b09850e-c231-440b-9642-231ef6815269)

![mini batch grad](https://github.com/user-attachments/assets/102dea14-958d-4091-985e-7e0967814fac)

#### Training with mini batch gradient descent

- The size of the mini-batch is an important parameter to consider.
- If the mini-batch size is equal to the size of the training set, it becomes batch gradient descent
- On the other hand, if the mini-batch size is equal to 1, it becomes **stochastic gradient descent**.
  - Disadvantage is that we loose all thespeeding from vectorization

_In practice, the mini-batch size is usually somewhere in between, not too big or too small, to balance efficiency and accuracy._

```
Small training set -> Batch gradient descent
Typical training set -> 64, 128, 256, 512 and 1024
```

### Exponentially weighted averages

- It's a way to calculate the average of a set of numbers, but with more emphasis on recent values.
- The weights decrease exponentially as we go back in time. So, the most recent data has the highest weight, and the weight decreases as we move further back in time.
- By using these weighted averages, we can smooth out the data and get a better sense of the overall trend in data.

`V(t) = Î² V(t-1) + (1 - Î²) Î¸(t)`

_V(t) is exponentially weighted average at time t, Î¸(t) is data at time t and Î² is a constant and V(0) = Î¸(0)_

#### Intuition

#### Bias correction

- Bias correction in exponentially weighted averages is a technique that helps improve the accuracy of your estimates, especially during the initial phase.
- To correct this bias, you divide the moving average by a correction factor `1 - Î²^t`.
- This correction factor depends on the value of Î² (a parameter used in the calculation) and the current day you're on.

### Gradient descent with momentum

- It's an algorithm that helps optimize a cost function in a more efficient way compared to the standard gradient descent algorithm.
- Gradient descent with momentum solves this problem by using a technique called `momentum`. It computes an exponentially weighted average of the gradients (which tell us the direction to move in) and uses that average to update the weights.

```
vdW = Î² vdW + (1 - Î²) dw -> W = W - learning_rate * vdW
vdb = Î² vdb + (1 - Î²) db -> b = b - learning_rate * vdb
```

### Adaptive gradient

- Adapting the step size for each input variable based on the gradients observed during optimization.
- It adjusts the learning rate individually for each parameter.
- For parameters with high gradients, It reduces the learning rate, preventing overshooting. Conversely, for parameters with low gradients, It increases the learning rate to speed up convergence.

_It works well even when the gradients vary significantly across dimensions._

### RMSprop

- RMSprop achieves this by keeping an exponentially weighted average of the squares of the derivatives of the parameters.
- It divides the updates in the vertical direction by a larger number (to dampen the oscillations) and the updates in the horizontal direction by a smaller number (to maintain learning speed).
- This helps to stabilize the learning process and allows for faster learning without diverging in the oscillating direction.

### Adam Optimizer

- Momentum helps you keep moving in the right direction by remembering your previous steps. RMSprop helps you adjust your steps based on the terrain, so you don't take too big or too small steps.
- Adam combines these two techniques to find the best path efficiently.
- It has been shown to work well in many different types of neural networks. By using Adam, you can train your neural networks more quickly and effectively.

```
Î± -> Need to be tune
Î²1 -> 0.9 (dw)
Î²2 -> 0.999 (dw)
Îµ -> 10^-8
```

`Î± = (1 / (1 + decayRate * epochNumber)) * ğ›¼(0)`
â€‹
