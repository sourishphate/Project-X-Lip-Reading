### Batch v/s mini-batch gradient descent:
- Batch Gradient Descent processes the entire training set before taking a single step in gradient descent, which can be slow for large datasets.
- Mini-batch gradient descent,is an optimization algorithm that can significantly speed up training for neural networks, especially when dealing with large datasets.
- In mini-batch gradient decent the training set is split into smaller "mini-batches"using vectorization to make the training process faster. Each mini-batch is processed independently, allowing for faster iterations.

![mini v batch](https://github.com/user-attachments/assets/2078295f-0c32-436b-bf21-379b715d4602)

- Processing Mini-Batches
  - For each mini-batch ` X^t, Y^t` :
   - 1. Implement forward propagation.
   - 2. Compute the cost function for the mini-batch.
   - 3. Implement backpropagation to compute gradients.
   - 4. Update weights  W  and biases b .

![mini batcch grad](https://github.com/user-attachments/assets/304f1645-1cfc-49e2-943e-7e8fa4eb1cb1)

### Training with mini batch gradient descent:
- The size of the mini-batch is an important parameter to consider.
- Batch Gradient Descent: When mini-batch size equals the training set size M, it's just batch gradient descent. Processing the entire training set on each iteration is time-consuming for large datasets.
- Stochastic Gradient Descent: When mini-batch size equals 1, each example is processed individually. This leads to high noise and inefficiency due to lack of vectorization, making it hard to converge.

- Ideally the size of the mini-batcj should be between 1 and M not too big or small.

```
Small training set -> Batch gradient descent
Typical training set -> 64, 128, 256, 512 and 1024
```
![mini batch size](https://github.com/user-attachments/assets/edac027f-e614-40f2-99a1-63c493e88f3c)

### Exponentially weighted averages:
- Exponentially weighted averages is an optimization method used to find trends and patterns in noisy datasets using the concept of moving averages.
- The algorithm gives more emphasis to recent data and less to older data.
- Formula:
  - `V_(t) = Œ≤ V_(t-1) + (1 - Œ≤) Œ∏_(t)`
  - Œ≤ : Between 0 and 1.
  - Œ∏: Value at time t .

  - **High Œ≤  (e.g., 0.98)**: Smoother, slower response, less noise.
  - **Moderate Œ≤  (e.g., 0.9)**: Balanced smoothness and responsiveness.
  - **Low Œ≤  (e.g., 0.5)**: Faster response, noisier.

![exp weight avg](https://github.com/user-attachments/assets/fd096eb8-0fae-4cc0-b66c-a447c28b3ba1)

#### Intuition and implementation:
![ewa intu](https://github.com/user-attachments/assets/eae45124-001e-49ab-aa88-1fcae0dca807)

![implement ewa](https://github.com/user-attachments/assets/138ad782-1132-4b24-804a-a57ab1aa44d5)

#### Bias correction:
- Bias correction is a technique used to improve the accuracy of exponentially weighted averages, especially during the initial phase when the estimate may be skewed or less accurate.
- When you first initialize the exponentially weighted average with zero, the early estimates can be biased low. Bias correction helps adjust these early estimates to be more accurate.
- To correct this bias, you divide the moving average by a correction factor `1 - Œ≤^t`.
- Bias Correction Formula: 
  - ` V(t_corrected) = V_(t)/1 - Œ≤^t`

```
- During the initial phase of the moving average, bias correction significantly improves the accuracy of the estimates.
- As  t  becomes large, the term Œ≤^t approaches zero, so the bias correction has less impact.
```

![bias correction](https://github.com/user-attachments/assets/0836d7bd-1c2f-421e-94a1-06cda12b892b)

### Gradient descent with momentum:
- The basic idea is to compute an exponentially weighted average of your gradients and then they use that gradient update your weights.
- Helps to reduce oscillations in the path towards the minimum of a cost function.
- Allows using a larger learning rate without risking divergence, leading to faster convergence.
- Momentum Intuition
  - Vertical and Horizontal Learning Rates: In a function with elongated contours, we want a slower learning rate in the vertical direction to reduce oscillations and a faster rate in the horizontal direction to move quickly towards the minimum.
  - Momentum helps by averaging out the gradients: This reduces oscillations and accelerates convergence in the desired direction.

![grad desc momen](https://github.com/user-attachments/assets/e34e82da-4c50-4e47-beaa-b7a28319214c)

```
vdW = Œ≤ vdW + (1 - Œ≤) dw -> W = W - learning_rate * vdW
vdb = Œ≤ vdb + (1 - Œ≤) db -> b = b - learning_rate * vdb
```
![grad momen imple](https://github.com/user-attachments/assets/cbd70e14-7789-4262-ac04-377a278b5139)

### RMSprop: Root Mean Square Propagation:
- RMSprop is an optimization algorithm designed to address the issue of oscillations in the vertical direction during gradient descent, helping the algorithm to converge faster and more smoothly.
- RMSprop modifies the standard gradient descent algorithm by adjusting the learning rates based on a moving average of the squared gradients.
- It divides the updates in the vertical direction by a larger number (to dampen the oscillations) and the updates in the horizontal direction by a smaller number (to maintain learning speed).
- This helps to stabilize the learning process and allows for faster learning without diverging in the oscillating direction.

![rmsprop](https://github.com/user-attachments/assets/1ebc47a6-c13d-4b65-af86-59dda57253e8)

### Adam Optimizer:
- The main idea of the Adaptive momentum estimation optimizer is to combine two techniques, Momentum and RMSprop, to make neural network training faster and more efficient.
- Momentum helps keep your training moving in the right direction by remembering the direction of your previous steps. This helps smooth out the path and prevents getting stuck.
- RMSprop adjusts your step size based on the terrain of the loss function. It prevents steps from being too large in steep areas and too small in flat areas.

- How Adam Works
  - Adam initializes two moving averages: one for the gradients and one for the squared gradients. During each iteration:
  - It updates these moving averages with the current gradients.
  - It adjusts the parameters using both the moving averages and the original gradients.
  - This combination allows Adam to adapt the learning rate for each parameter

![adam1](https://github.com/user-attachments/assets/fb069dcf-34ec-4b11-81c8-364e77095a91)

![adam2](https://github.com/user-attachments/assets/14ffff06-86b3-4ca9-8b42-f689a8f80a06)

### Learning rate decay:
- Learning rate decay is a technique that helps speed up your learning algorithm by gradually reducing the learning rate over time.
- Begin training with a high learning rate to enable the model to learn quickly and explore various solutions. As training progresses, gradually reduce the learning rate. 
- This approach allows the model to make smaller, more refined updates to its parameters. By doing so, the model can fine-tune its learning and is less likely to get trapped in poor solutions.

`Œ± = (1 / (1 + decay_rate * epoch_num)) * ùõº(0)`

![learning decay](https://github.com/user-attachments/assets/9566c537-3390-4f0d-9627-3ddedc7cff64)

#### Other learning rate decay

1. `Œ± = constant / sqrt(epoch_num) * Œ±(0)`
2. `Œ± = Œ±(0) * e^(-decay_rate * epoch_num)`
3. `Œ± = Œ±(0) * decay_rate^(floor(epoch_num / decay_steps))`
