### 1. Hyperparameter

- The `learning rate`, which determines how quickly the network learns, is usually the most important one.
- Other important hyperparameters include the `momentum term`, `mini-batch size`, `number of hidden units` and `learning rate decay`.

#### 1.1 Tuning process

- Instead of trying out all possible combinations of hyperparameter values, it's better to randomly sample a set of values to explore.
- This allows us to try out a wider range of values for the most important hyperparameters.
- We can also use a coarse to fine sampling scheme, where we start with a broad search and then zoom in on promising regions.
- By systematically exploring different hyperparameter values, we can find the best settings that optimize the network's performance

#### 1.2 Using appropriate scale

- When sampling hyperparameters, it's important to pick the appropriate scale to explore the values.
- If we are choosing the number of hidden units and we think a good range is from 50 to 100, we can sample uniformly at random within this range.
- But for other hyperparameters, like the learning rate, sampling uniformly at random may not be the best approach.
- Instead, we can use a logarithmic scale to sample values. This means that we sample more values in the range where the hyperparameter is close to its maximum or minimum value.

_This is because small changes in these values can have a big impact on the results. By using a logarithmic scale, we can distribute our samples more efficiently and explore the space of possible values in a better way._

#### 1.3 Approaches

1. **Babysitting a single model:** train the model and monitor its performance, making small adjustments to the hyperparameters along the way.

_This approach is suitable when computational resources are limited._

2. **Train multiple models:** training multiple models in parallel with different hyperparameter settings and compare their performance to choose the best one.

_This approach is feasible when you have enough computational resources._
