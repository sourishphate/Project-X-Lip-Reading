### 1. Normalizing Inputs

- Normalizing means making sure that the input features have similar scales or ranges.

  - We subtract the mean (average) value from each feature, so that the features have a mean of zero.
  - Dividing each feature by its standard deviation to normalize the variances.


### 2. Vanishing / Exploding Gradient
  
- When these weight matrices are initialized to be slightly larger than the identity matrix, the output of the network grows exponentially with the number of layers. This is known as the problem of exploding gradients.

- If the weight matrices are initialized to be slightly smaller than the identity matrix, the output of the network decreases exponentially with the number of layers. This is known as the problem of vanishing gradients.

*However, careful initialization of the weights can help mitigate these issues to some extent.*

![vanish](https://github.com/user-attachments/assets/0ffd3c41-4d7e-4c5a-97d2-8790183ff4ea)

### 3. Weight Initialization

- When we train a neural network, we need to set the initial values for the weights. If we choose the wrong values, it can cause problems like the gradients becoming too big or too small.
- When we train a neural network, we need to set the initial values for the weights. If we choose the wrong values, it can cause problems like the gradients becoming too big or too small.
- One common approach is to randomly initialize the weights using a Gaussian distribution. 

  - If we have a neuron with 4 input features, we can set the variance of the weights to be `1/4` 
  - If we're using a ReLU activation function, it's even better to set the variance to be `2/n`, where n is the number of input features.
  - With the tanh activation function, it has been found that setting the variance of the weights to `1/n`

![weight initialization](https://github.com/user-attachments/assets/540e91cd-90bd-452c-b02b-bb8dbc30c892)

### 4. Numerical Approxiation of Gradients

Let's assume we have a function f(θ) and we want to calculate its derivative, denoted as g(θ). Here's the formula for gradient checking:

`g(θ) ≈ (f(θ + ε) - f(θ - ε)) / (2 * ε)`

### 5. Gradient checking

- **Reshaping parameters:** In our neural network, we have parameters like weights (W) and biases (B). To perform gradient checking, we reshape these parameters into a single vector called theta.
- **Computing the cost function:** The cost function (J) measures how well our neural network is performing. With gradient checking, we treat J as a function of theta instead of individual parameters.
- **Approximating the derivatives:** We want to check if the derivatives of J with respect to theta (d theta) are correct. To do this, we compute an approximation of d theta for each component of theta using a two-sided difference.
- **Comparing the vectors:** We compare the computed d theta approximations with the actual derivatives d theta. If they are approximately equal, it means our derivative approximation is likely correct.

![gradient checking](https://github.com/user-attachments/assets/2a2a2f8b-1e11-420a-ac0f-475e75b90b5f)

![grad check](https://github.com/user-attachments/assets/5896fafb-6408-4d54-a769-9426e175038d)

#### 5.1 Implementation notes

- Don't use in training - only to debug
- If algorithm fails grad check, look at components to try to identify bug.
- Remember regularization.
- Doesn't work with dropout.
- Run at ramdom initialization, perhaps again after some training.