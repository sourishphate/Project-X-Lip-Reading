## Regularization

- It's a technique used in machine learning to prevent a common problem called overfitting.

  _Overfitting happens when a model becomes too complex and starts to memorize the training data instead of learning the underlying patterns._

### 1. Logistic Regression

- In logistic regression, we try to minimize a cost function called J. To add regularization, we introduce a parameter called lambda.
- Regularization is applied to the parameter w. We add a term to the cost function that includes lambda, the norm of w squared, and some other constants.
- This term helps to make the parameter values smaller, reducing the complexity of the model.
- It's like adding a constraint that encourages the model to focus on the important patterns in the data rather than memorizing every detail.

```
J = (1/m) * sum(-y * log(h(x)) - (1-y) * log(1 - h(x))) + (lambda/(2*m)) * sum(w^2)
```

![Logistic regression](https://github.com/user-attachments/assets/96699bdd-00ef-4147-9943-372f8ebc9c9c)

### 2. Neural Network

![neural network](https://github.com/user-attachments/assets/03e959bc-19f8-4561-bbc9-fc815e4afa72)

**Why regularization reduces overfitting?**

Imagine you are trying to fit a puzzle together. If you have too many puzzle pieces, it becomes difficult to find the right fit, and you might end up forcing pieces together that don't actually belong. This is similar to overfitting in machine learning.

_Regularization acts like a constraint that limits the complexity of the puzzle, making it easier to find the correct fit._

![regula](https://github.com/user-attachments/assets/9025dfbd-ab55-4407-b5b2-fa25c8d3b433)

### 3. Dropout Regularization

- Dropout works by randomly removing some nodes (or neurons) from each layer of the network during training.
- This creates a smaller and less complex network.
- By training the network with these smaller, randomly modified versions, dropout helps to regularize the network and prevent overfitting.
- It's like training multiple different networks on different subsets of the data.

![dropout](https://github.com/user-attachments/assets/7ff813c8-13c4-481b-83e0-abbcea54e36b)

**Why does dropout works**

- We randomly remove some workers from the team on each iteration. This means that the team becomes smaller, and each worker has to be more versatile and not rely too much on any one task.
- This helps prevent the team from becoming too specialized and ensures that they can handle different tasks well.

![inverted drop](https://github.com/user-attachments/assets/0464cf4c-34e0-42cf-9108-89d21b19c10a)

### 4. Other Regularization Methods

#### 4.1 Data Augmentation

- Data augmentation is a technique used to increase the size of a training dataset by creating additional fake examples.
- This helps to reduce overfitting in a neural network.

_By using data augmentation, you are essentially telling the neural network that if something is a cat, flipping it horizontally or zooming in on a part of the image should still be recognized as a cat._

#### 4.2 Early Stopping

- During the training process, the performance of the model is monitored on a separate validation set.
- Early stopping works by stopping the training process when the performance on the validation set starts to deteriorate.
- By stopping the training early, we prevent the model from overfitting and ensure that it generalizes well to new, unseen data.
