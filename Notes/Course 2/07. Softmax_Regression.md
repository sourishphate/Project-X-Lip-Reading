### Softmax Regression

We use a neural network with an upper layer that has four units, one for each category. Each unit gives us the probability of the image belonging to that category. The probabilities should add up to 1. We use a special activation function called Softmax to calculate these probabilities.

1. We calculate a temporary variable called t by taking the exponential of the values in the upper layer.
2. Then, we normalize the values in t so that they add up to 1. This gives us the probabilities for each category.

*For example, if the image is a cat, the unit for class 1 will have a high probability, while the units for the other classes will have lower probabilities.*

![softmax layer](https://github.com/user-attachments/assets/99b505c6-64df-4012-8c0e-91a506dea57c)

#### Training a softmax classifier

1. **Softmax function:** It takes the output values from the previous step and calculates the probabilities for each category. It makes sure that all the probabilities add up to 1.
2. **Softmax vs. Hard max:** Softmax is like a gentle way of assigning probabilities to categories based on the input values. On the other hand, hard max is more strict and only assigns a 1 to the category with the highest value and 0 to the rest.
3. **Generalization of logistic regression:** Softmax classification is an extension of logistic regression to more than two categories. If there are only two categories, softmax classification is the same as logistic regression.

![loss function](https://github.com/user-attachments/assets/c169a7e5-c082-4917-a98b-2ed2b3a36d0c)
