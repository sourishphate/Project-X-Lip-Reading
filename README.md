<h1 align="center"> Lip Reading ğŸ’¬</h1>


<p align="center">
  <img src="https://github.com/user-attachments/assets/9052f2a1-c027-401e-a79f-e3835189365b" alt="read-my-lips" width="350" />
</p>


<div align="center">
  <strong>The goal is to convert visual information from lip movements into text. This involves recognizing and interpreting the movements of a speaker's lips to accurately transcribe spoken words.</strong>
</div>

<br />

## ğŸ“Š Results


## ğŸ“‘ Table of Contents

- [About the Project](#-about-the-project)
- [Tech Stack](#ï¸-tech-stack)
- [File Structure](#-file-structure)

## ğŸ“˜ About the Project


## âš™ï¸ Tech Stack

| **Category**                | **Technologies**                                                                                       |
|-----------------------------|----------------------------------------------------------------------------------------------------|
| **Programming Languages**   | [![Python](https://img.shields.io/badge/python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)              |
| **Frameworks**              | [![TensorFlow](https://img.shields.io/badge/tensorflow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://www.tensorflow.org/) [![Keras](https://img.shields.io/badge/keras-D00000?style=for-the-badge&logo=keras&logoColor=white)](https://keras.io/) |
| **Libraries**               | [![OpenCV](https://img.shields.io/badge/opencv-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/) [![NumPy](https://img.shields.io/badge/numpy-013243?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)             |
| **Deep Learning Models**    | [![LSTM](https://img.shields.io/badge/LSTM-563D7C?style=for-the-badge&logo=lstm&logoColor=white)](https://en.wikipedia.org/wiki/Long_short-term_memory) [![CNN](https://img.shields.io/badge/CNN-0A192E?style=for-the-badge&logo=cnn&logoColor=white)](https://www.geeksforgeeks.org/convolutional-neural-network-cnn-in-machine-learning/) |
| **Dataset**                 | [![MIRACL-VC1](https://img.shields.io/badge/MIRACL--VC1-4D2A4E?style=for-the-badge&logo=dataset&logoColor=white)](https://paperswithcode.com/dataset/miracl-vc1)                                                                            |
| **Tools**                   | [![Git](https://img.shields.io/badge/git-F05032?style=for-the-badge&logo=git&logoColor=white)](https://git-scm.com/) [![Google Colab](https://img.shields.io/badge/google%20colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/)                            |
| **Visualization & Analysis**| [![Matplotlib](https://img.shields.io/badge/matplotlib-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://matplotlib.org/) [![Seaborn](https://img.shields.io/badge/seaborn-013243?style=for-the-badge&logo=python&logoColor=white)](https://seaborn.pydata.org/)                 |


## ğŸ“ File Structure

    â”œâ”€â”€ Dataset Preprocessing
       â”œâ”€â”€ xml files
          â”œâ”€â”€ haarcascade_frontalface_default.xml
          â”œâ”€â”€ haarcascade_mcs_mouth.xml
          â”œâ”€â”€ shape_predictor_68_face_landmarks.dat
       â”œâ”€â”€ 01_Face_Extraction.ipynb
       â”œâ”€â”€ 02_Lip_Extraction.ipynb
       â”œâ”€â”€ 03_Train_Test_Split.ipynb
    â”œâ”€â”€ Hyperparameter Tuning
       â”œâ”€â”€ Grid Search.ipynb
       â”œâ”€â”€ Random Search.ipynb
    â”œâ”€â”€ Mini Projects
       â”œâ”€â”€ Cat_Dog_Classifier_CNN.ipynb
       â”œâ”€â”€ Human_Action_Recognition_LSTM.ipynb
       â”œâ”€â”€ Next_Word_Predictor_LSTM.ipynb
       â”œâ”€â”€ Video_Anomaly_Detection_CNN_LSTM.ipynb
    â”œâ”€â”€ Model Architecture
       â”œâ”€â”€ Saved Model
          â”œâ”€â”€ 3D_CNN_Bi-LSTM.h5
       â”œâ”€â”€ 3D_CNN.ipynb
       â”œâ”€â”€ 3D_CNN_Bi-LSTM.ipynb
       â”œâ”€â”€ 3D_CNN_From_Scratch.ipynb
       â”œâ”€â”€ 3D_CNN_LSTM.ipynb
       â”œâ”€â”€ Adam.ipynb
       â”œâ”€â”€ CategoricalCrossentropy.ipynb
       â”œâ”€â”€ Data_Augmentation.ipynb
       â”œâ”€â”€ Dropout.ipynb
       â”œâ”€â”€ EarlyStopping.ipynb
       â”œâ”€â”€ L1_Regularization.ipynb
       â”œâ”€â”€ L2_Regularization_1.ipynb
       â”œâ”€â”€ L2_Regularization_2.ipynb
       â”œâ”€â”€ RMSprop.ipynb
    â”œâ”€â”€ Model Evaluation
       â”œâ”€â”€ Accuracy.ipynb
       â”œâ”€â”€ Live_Detection.ipynb
       â”œâ”€â”€ Onlne_Testing.ipynb
       â”œâ”€â”€ Precision.ipynb
       â”œâ”€â”€ Recall.ipynb
    â”œâ”€â”€ Notes
       â”œâ”€â”€ LSTM
       â”œâ”€â”€ OpenCV
       â”œâ”€â”€ Om Mukherjee
       â”œâ”€â”€ Sourish Phate       
    â”œâ”€â”€ README.md

## ğŸ’¾ Dataset: MIRACL-VC1

The **MIRACL-VC1** dataset is structured to facilitate research in visual speech recognition, particularly lip reading. Here's a breakdown of its structure and contents:

#### Data Composition:
- **Video Clips**: The dataset contains short video clips of multiple speakers reciting specific phrases. Each clip captures the upper body, focusing mainly on the face and mouth area.
- **Speakers**: It features several speakers from diverse backgrounds, which helps models generalize across different individuals and speaking styles.
- **Languages**: The dataset is typically in English, though speakers may vary in accents and pronunciations.
- **Phrases**: Each video clip corresponds to one of a predefined set of phrases, which are recited by the speakers. The phrases are usually short and may cover simple daily expressions or numbers.

#### Dataset Contains The Following Words and Phrases:

![image](https://github.com/user-attachments/assets/cc4d4ba3-0961-4439-8a36-0a0de73e2bac)

[Download the MIRACL-VC1 dataset on Kaggle](https://www.kaggle.com/datasets/apoorvwatsky/miraclvc1)

